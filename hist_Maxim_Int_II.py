import requests
import json
import scrapy
import re
from scrapy.http import FormRequest
from scrapy_splash import SplashRequest
from w3lib.http import basic_auth_header
from scrapy.selector import Selector
from swisscom_IV_crawler.items import SwisscomIvCrawlerItem

### Maxim Integrated Products Inc. 2|2
### get with json, all content comes with one request; 
### for part of the news detailpage comes as json, 
### goes back to 19951130
### 45 errors seem to be generated by broken links

class BHGE(scrapy.Spider):
    name = "Maxim_Int_II_9698000ARV002"
    
    custom_settings = {
         'JOBDIR' : 'None',
         'FILES_STORE' : 's3://352569/Maxim_Int_II_9698000ARV002/',
        }
    
    start_urls = ['https://www.maximintegrated.com/en/appservice/productservice/_jcr_content.loadprarticles.json'] 
 
    def parse(self, response):
        body = json.loads(response.text)  # load jason response from post request
        #body = dat[-1]['data']  # [-1] selects last element # extract data body with html content from the json response file
        #quotes = Selector(text=body).xpath('//div[@class="views-row"]')  # define html body content as reference for the selector
        for dat in body['items']:
            item = SwisscomIvCrawlerItem()
                       # item['file_urls'] = [url]
            item['PUBSTRING'] = dat['date']
            item['HEADLINE']= dat['title']
            
            if dat['externallink'] == '':
                item['DOCLINK']= dat['path']
                base_url = 'https://www.maximintegrated.com/en/appservice/productservice/_jcr_content.loadonearticledetails.json/'
                aux_url =  dat['path']
                url= base_url + aux_url
                request = scrapy.Request(url=url, callback=self.parse_details_json)
                request.meta['item'] = item
                yield request
            
            else:    
                item['DOCLINK']= dat['externallink']
                #item = {
                #          'PUBSTRING': dat['date'],
                #          'HEADLINE': dat['title'],
                #          'DOCLINK': dat['externallink'],
                #         }
                base_url = 'https://investor.maximintegrated.com'
                aux_url =  dat['externallink']
                
                #url= base_url + dat['externallink']
                if aux_url.startswith('http'):
                    url= aux_url
                    if ".pdf" not in url.lower(): # make url all lowercase so match is not casinsensitive anymore
                        request = scrapy.Request(url=url, callback=self.parse_details)
                        request.meta['item'] = item
                        yield request
        
                    else:
                        item = SwisscomIvCrawlerItem()
                        item['file_urls'] = [url]
                        item['PUBSTRING'] = dat['date']
                        item['HEADLINE']= dat['title']
                        item['DOCLINK']= url
                        yield item 
                else:
                    url= base_url + aux_url
                    if ".pdf" not in url.lower(): # make url all lowercase so match is not casinsensitive anymore
                        request = scrapy.Request(url=url, callback=self.parse_details)
                        request.meta['item'] = item
                        yield request
    
                    else:
                        item = SwisscomIvCrawlerItem()
                        item['file_urls'] = [url]
                        item['PUBSTRING'] = dat['date']
                        item['HEADLINE']= dat['title']
                        item['DOCLINK']= url
                        yield item


    def parse_details(self, response):
        item = response.meta['item']
        name_regex = r'(Forward(.|\s*)Looking\s*Statements)(.|\s)*'
        name_regex_2=r'(\bAbout\s*Maxim)(.|\s)*|(\bAbout.Maxim\b)(.|\s)*|(\bABOUT.Maxim\b)(.|\s)*|(\bABOUT\s*.MAXIM\b)(.|\s)*'
        item['DESCRIPTION'] = re.sub(name_regex,'' ," ".join(response.xpath('//div[@class="ModuleBody"]//text()[not(ancestor::div[@class="box__right"] or self::style or self::script or  ancestor::style or ancestor::script or ancestor::p[@id="news-body-cta"] or ancestor::div[@id="bwbodyimg"])]').extract()))
        item['DESCRIPTION'] = re.sub(name_regex_2,'' , item['DESCRIPTION'])
        item['DOCLINK'] = response.url
        #yield item
        if not item['DESCRIPTION']:
            item['DESCRIPTION'] = re.sub(name_regex,'' ," ".join(response.xpath('//text()[not(ancestor::div[@class="box__right"] or self::style or self::script or  ancestor::style or ancestor::script or ancestor::p[@id="news-body-cta"] or ancestor::div[@id="bwbodyimg"])]').extract()))
            item['DESCRIPTION'] = re.sub(name_regex_2,'' , item['DESCRIPTION'])
            yield item
        else:     
            yield item
           
    def parse_details_json(self, response):
        item = response.meta['item']
        name_regex = r'(Forward(.|\s*)Looking\s*Statements)(.|\s)*'
        name_regex_2=r'(\bAbout\s*Maxim\s*Integrated)(.|\s)*|(\bAbout.Maxim.Integrated\b)(.|\s)*|(\bABOUT.Maxim.Integrated\b)(.|\s)*|(\bABOUT\s*.MAXIM\s*INTEGRATED\b)(.|\s)*'
        body = json.loads(response.text)
        data = body['article']['text']
        #Selector(text=body).xpath('//div[@class="views-row"]')
        item['DESCRIPTION'] = re.sub(name_regex,'' ," ".join(Selector(text=data).xpath('//text()[not(ancestor::div[@class="box__right"] or self::style or self::script or  ancestor::style or ancestor::script or ancestor::p[@id="news-body-cta"] or ancestor::div[@id="bwbodyimg"])]').extract()))
        item['DESCRIPTION'] = re.sub(name_regex_2,'' , item['DESCRIPTION'])
        item['DOCLINK'] = response.url
        if not item['DESCRIPTION'] or "NULL" in item['DESCRIPTION'] :
            url_pdf = body['article']['pdf']
            base_url = 'https://www.maximintegrated.com'
            item['file_urls'] = [base_url + url_pdf]
            item['DOCLINK'] = base_url + url_pdf
            yield item
        else:     
            yield item



        
            